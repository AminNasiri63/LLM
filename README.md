# LLM-Learning Repository
This repository contains a collection of projects and experiments developed while learning about **large language models (LLMs)**. It includes hands-on implementations using **Hugging Face Transformers**, covering topics such as:
* Prompt engineering
* In-context learning (zero-shot, one-shot, few-shot)
* Use FLAN-T5, a large language model fine-tuned with instruction-based tasks to follow natural language prompts
* Use mBART-50, a multilingual sequence-to-sequence model trained on many languages
* Translate input text
* Evaluation using BLEU score

---

# üìù SummarizeDialogue.ipynb
* This file uses the **FLAN-T5 model** and **Hugging Face** to experiment with prompt engineering and in-context learning, including zero-shot, one-shot, and few-shot scenarios.

---

# üìù DialogueToFarsi.ipynb
* This file provides a simple example of translating text from Farsi (Persian) to English using the **facebook/mbart model** from **Hugging Face Transformers**.

---

# üìù BleuScore.ipynb
* This file compares English-to-Farsi translation quality between **facebook/mbart model** and **Google Translate**, using **BLEU score** as the evaluation metric.

---

# üìù RAGLangChain.ipynb
* This Jupyter Notebook demonstrates a complete **Retrieval-Augmented Generation (RAG)** pipeline built with **LangChain**. It's specifically configured to answer questions in Persian based on the content of a provided PDF document.

---

