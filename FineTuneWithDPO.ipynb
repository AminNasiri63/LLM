{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UTfdFzEqN0-v",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTfdFzEqN0-v",
        "outputId": "97b75755-6c2f-455b-e492-f0cc4e89a8fc"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install --no-deps git+https://github.com/lvwerra/trl.git\n",
        "%pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6b288de-ec97-4cda-bba2-d35fcbc62f8b",
      "metadata": {
        "id": "d6b288de-ec97-4cda-bba2-d35fcbc62f8b"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from datasets import load_dataset\n",
        "from trl import DPOConfig, DPOTrainer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bb6e848-eaee-4fd7-8a86-8dc1dcc87711",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bb6e848-eaee-4fd7-8a86-8dc1dcc87711",
        "outputId": "45a21d86-bbe7-4843-8d67-22eb78708500"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"shawhin/youtube-titles-dpo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "558119ea-32a7-4125-84f6-20463821ea3d",
      "metadata": {
        "id": "558119ea-32a7-4125-84f6-20463821ea3d"
      },
      "source": [
        "### load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6e7c0fb-7b79-4b1f-8859-e822f087da21",
      "metadata": {
        "id": "a6e7c0fb-7b79-4b1f-8859-e822f087da21"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained model and tokenizer\n",
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token # set pad token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a2d42dc-7eca-4007-aa16-0eee1049f49d",
      "metadata": {
        "id": "7a2d42dc-7eca-4007-aa16-0eee1049f49d"
      },
      "outputs": [],
      "source": [
        "# Define the chat prompt formatting function\n",
        "def format_chat_prompt(user_input, system_message=\"You are a helpful assistant.\"):\n",
        "    \"\"\"\n",
        "    Formats user input into the chat template format with <|im_start|> and <|im_end|> tags.\n",
        "\n",
        "    Args:\n",
        "        user_input (str): The input text from the user.\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted prompt for the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Format user message\n",
        "    user_prompt = f\"<|im_start|>user\\n{user_input}<|im_end|>\\n\"\n",
        "\n",
        "    # Start assistant's turn\n",
        "    assistant_prompt = \"<|im_start|>assistant\\n\"\n",
        "\n",
        "    # Combine prompts\n",
        "    formatted_prompt = user_prompt + assistant_prompt\n",
        "\n",
        "    return formatted_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6535198b-d0ce-4e48-9681-14ed678d55ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6535198b-d0ce-4e48-9681-14ed678d55ea",
        "outputId": "f4ebfeee-ca2f-45e4-fba1-d75571d95793"
      },
      "outputs": [],
      "source": [
        "# Set up text generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device='cuda')\n",
        "\n",
        "# Example prompt\n",
        "prompt = format_chat_prompt(dataset['valid']['prompt'][0][0]['content'])\n",
        "\n",
        "# Generate output\n",
        "outputs = generator(prompt, max_length=100, truncation=True, num_return_sequences=1, temperature=0.7)\n",
        "\n",
        "print(outputs[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6b4e8a7-8860-4726-8783-437e10072e3c",
      "metadata": {
        "id": "d6b4e8a7-8860-4726-8783-437e10072e3c"
      },
      "outputs": [],
      "source": [
        "# Fine-tuning configuration\n",
        "ft_model_name = model_name.split('/')[1].replace(\"Instruct\", \"DPO\")\n",
        "\n",
        "training_args = DPOConfig(\n",
        "    output_dir=ft_model_name,\n",
        "    logging_steps=25,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    bf16=True,\n",
        "    num_train_epochs=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    eval_steps=1,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-jsywUCLfO5M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "-jsywUCLfO5M",
        "outputId": "6b9225c9-2e6c-427b-c095-a2a1438cf737"
      },
      "outputs": [],
      "source": [
        "# Initialize the DPO Trainer\n",
        "trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['valid'],\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0gJBy3o-fRVh",
      "metadata": {
        "id": "0gJBy3o-fRVh"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned model\n",
        "ft_model = trainer.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47134052-877c-4ec8-8c87-18a5827f7a31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47134052-877c-4ec8-8c87-18a5827f7a31",
        "outputId": "5c84a85d-0bfd-43a5-9751-cfbaf0feaa81"
      },
      "outputs": [],
      "source": [
        "# Set up text generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=ft_model, tokenizer=tokenizer, device='cuda')\n",
        "\n",
        "# Example prompt\n",
        "prompt = format_chat_prompt(dataset['valid']['prompt'][0][0]['content'])\n",
        "\n",
        "# Generate output\n",
        "outputs = generator(prompt, max_length=100, truncation=True, num_return_sequences=1, temperature=0.7)\n",
        "\n",
        "print(outputs[0]['generated_text'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "LLM1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
